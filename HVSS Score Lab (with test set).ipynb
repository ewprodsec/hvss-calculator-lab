{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "033b90ed",
   "metadata": {},
   "source": [
    "# **HVSS Machine Learning Lab (with test set)**\n",
    "\n",
    "**Welcome to the HVSS AI/ML lab!**\n",
    "\n",
    "This lab is designed to help you get started with AI/ML model creation and to provide you with a framework for building and imporving the various HVSS models.\n",
    "\n",
    "***Notice: This lab includes a test step of the models' accuracy (MSE) using test sets, which are taken out of the training data. If you wish to train the models using the full data set (skipping the test step), please use  the 'HVSS Score Lab (full training data without test set).ipynb' file instead***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "177878df",
   "metadata": {},
   "source": [
    "### **Import all common libraries**\n",
    "\n",
    "First, we will import all the common libraries that we will use in this lab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4484d3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8091a60e",
   "metadata": {},
   "source": [
    "### **Loading the training data**\n",
    "\n",
    "We are managing the training data in **`TrainingData.xlsx`**, which contains multiple sheets for each of the different training data sets (Exploitability, XCIA, XHB, XPS, XSD). We will use the `pandas` library to read the data from each sheet in the Excel file into a separate `DataFrame` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993dbfb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an array of sheets\n",
    "sheets = ['Exploitability', 'XCIA', 'XPS', 'XSD', 'XHB']\n",
    "\n",
    "# Create a dictionary to hold the dataframes\n",
    "dfs = {}\n",
    "\n",
    "# Loop through the sheets and read its contents into a separate DataFrame.\n",
    "# Omitting the header names as they are not needed\n",
    "for sheet in sheets:\n",
    "    df = pd.read_excel('TrainingData.xlsx', sheet_name=sheet, header=None, skiprows=1)\n",
    "    \n",
    "    # Drop rows containing at least one NaN value\n",
    "    df_cleaned = df.dropna()\n",
    "    \n",
    "    dfs[sheet] = df_cleaned"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b79884a1",
   "metadata": {},
   "source": [
    "Let's print out the Exploitability and XCIA training data to see what it looks like. It will show you the first and last 5 rows of the data for each sheet (without the original table header):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c8e84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the Exploitability DataFrame\n",
    "print('Exploitability DataFrame:\\n')\n",
    "print(dfs['Exploitability'])\n",
    "\n",
    "# Display the XCIA DataFrame\n",
    "print('XCIA DataFrame:\\n')\n",
    "print(dfs['XCIA'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d957d564",
   "metadata": {},
   "source": [
    "Open the Exploitability and XCIA sheets in the Excel file and compare the original data with the above. Data should be the same."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6f3f424c",
   "metadata": {},
   "source": [
    "### **Preparing the training data**\n",
    "\n",
    "In order to train the model using the training data we have just loaded, we need to prepare the data first so that it can be properly consumed by the algorithm for generating the models. We will use the `scikit-learn` library to do this. The `scikit-learn` library is a very popular machine learning library for Python. It provides a lot of useful functions for data preparation, model training, and model evaluation.\n",
    "\n",
    "We need to split the training data into two parts: The input data, or \"features\" (marked by x) and the prediction data, or \"target\" (marked by ŷ - pronounced \"y hat\"). In our case, the features are the HVSS metrices values (e.g. XHB, XPS, XSD, etc.) and the target is the score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336b0c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionaries to hold the features and target variables for each sheet\n",
    "dfs_X = {}\n",
    "dfs_Y = {}\n",
    "\n",
    "# Loop through the sheets and split the features and target variables\n",
    "for sheet in sheets:\n",
    "    dfs_X[sheet] = dfs[sheet].iloc[:, :-1].values\n",
    "    dfs_Y[sheet] = dfs[sheet].iloc[:, -1].values"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c79595f4",
   "metadata": {},
   "source": [
    "Let's print the splitted XCIA data to see how it looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edcd98ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the XCIA feature values\n",
    "print(\"XCIA feature values (X):\\n\")\n",
    "print(dfs_X['XCIA'])\n",
    "print(\"\\n\")\n",
    "\n",
    "# Display the XCIA target value\n",
    "print(\"XCIA target value (Y):\\n\")\n",
    "print(dfs_Y['XCIA'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f65e2997",
   "metadata": {},
   "source": [
    "Since we have a small-sized data set, we will rely on the `GridSearchCV` function (down below) to perform the validation for us. Therefore, we will not need to split the data into a validation set. However, we will still need to split the data into a training set and a test set. The test set will be used to evaluate the final models. We will use the `train_test_split` function again to split the data into a training set and a test set: \n",
    "\n",
    "* 90% training set\n",
    "* 10% test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520c3ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Create dictionaries to hold the training and temporary sets for each sheet\n",
    "dfs_X_train = {}\n",
    "dfs_Y_train = {}\n",
    "dfs_X_test = {}\n",
    "dfs_Y_test = {}\n",
    "\n",
    "# Loop through the sheets and split the data set into a training set and a test set using an 90-10 split\n",
    "# The random state is set to 42 to ensure the same split is used for each sheet\n",
    "for sheet in sheets:    \n",
    "    dfs_X_train[sheet], dfs_X_test[sheet], dfs_Y_train[sheet], dfs_Y_test[sheet] = train_test_split(dfs_X[sheet], dfs_Y[sheet], test_size=0.1, random_state=42)\n",
    "\n",
    "\n",
    "# Display the XCIA training sub set\n",
    "print(\"XCIA training sub set:\\n\")\n",
    "print(dfs_X_train['XCIA'])\n",
    "print(\"\\n\")\n",
    "\n",
    "# Display the XCIA test sub set value\n",
    "print(\"XCIA test sub set:\\n\")\n",
    "print(dfs_X_test['XCIA'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "14043b5f",
   "metadata": {},
   "source": [
    "### **Finding the best algorithm parameters**\n",
    "\n",
    "Each AI/ML algorithm comes with a set of parameters that can be used to tune the algorithm (and effectively generate a more accurate model). For instance, in scikit-lean, the `MLPRegressor` (which is a neural network algorithm) has the following parameters:\n",
    "\n",
    "* `hidden_layer_sizes`: A tuple that defines the number of hidden layers and the number of neurons in each layer\n",
    "* `activation` :The activation function for the hidden layers\n",
    "* `solver`: The solver for weight optimization\n",
    "* `alpha`: The L2 penalty (regularization term) parameter\n",
    "* `batch_size`: The size of minibatches for stochastic optimizers\n",
    "* `learning_rate`: The learning rate schedule for weight updates\n",
    "* `learning_rate_init`: The initial learning rate used\n",
    "* `power_t`: The exponent for inverse scaling learning rate\n",
    "* `max_iter`: The maximum number of iterations\n",
    "* `shuffle`: Whether to shuffle samples in each iteration\n",
    "* `random_state`: The random number generator\n",
    "* `tol`: The tolerance for the optimization\n",
    "* `verbose`: Whether to print progress messages\n",
    "* `warm_start`: Whether to reuse the solution of the previous call to fit as initialization\n",
    "* `momentum`: The momentum for gradient descent update\n",
    "* `nesterovs_momentum`: Whether to use Nesterov’s momentum\n",
    "* `early_stopping`: Whether to use early stopping to terminate training when validation score is not improving\n",
    "* `validation_fraction`: The proportion of training data to set aside as validation set for early stopping\n",
    "* `beta_1`: The exponential decay rate for estimates of first moment vector in adam\n",
    "* `beta_2`: The exponential decay rate for estimates of second moment vector in adam\n",
    "* `epsilon`: The value for numerical stability in adam\n",
    "* `n_iter_no_change`: The maximum number of epochs to not meet tol improvement\n",
    "\n",
    "The documentation for the `MLPRegressor` algorithm can be found here: https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html\n",
    "\n",
    "As you can see, there are a lot of parameters that can be used to tune the algorithm. The question is, how do we know which parameters to use? The answer is, we don't. We need to try different combinations of parameters and see which one gives us the best result. This process is called \"hyperparameter tuning\". In this lab, we will use the `GridSearchCV` function from `scikit-learn` to perform hyperparameter tuning. The `GridSearchCV` function will try all the combinations of parameters that we specify and return the best combination. We will use the `GridSearchCV` function to find the best parameters for the `MLPRegressor` algorithm, for a subset of the optional parameters available.\n",
    "\n",
    "The model will be trained with the best parameters found by the `GridSearchCV` function.\n",
    "\n",
    "Notice that the `GridSearchCV` function will take few minutes to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cdf2b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "# Specify the hyperparameters and their values\n",
    "param_grid = {\n",
    "    # Single hidden layer is defined for the sake of simplicity. The value defines the # of neurons in the hidden layer.\n",
    "    # In order to use a multi-hidden-layer networks, use a tuple of integers [for instance (50, 100, 50) defines a network with 3 hidden layers, with 50, 100 and 50 neurons in each layer]\n",
    "    'hidden_layer_sizes': [(50,), (100,), (150,), (200,), (250,), (300,)], \n",
    "    'activation': ['relu', 'tanh', 'logistic'],\n",
    "    'solver': ['lbfgs', 'sgd', 'adam'],\n",
    "    'learning_rate': ['constant', 'adaptive'],    \n",
    "    'random_state': [42]  # setting the random state to a specific value ensures reproducibility\n",
    "}\n",
    "\n",
    "grid_search = {}\n",
    "\n",
    "for sheet in sheets:\n",
    "\n",
    "    # We'll use mean squared error as our scoring metric\n",
    "    grid_search[sheet] = GridSearchCV(MLPRegressor(max_iter=25000), param_grid, cv=5, scoring='neg_mean_squared_error', n_jobs=-1, verbose=1)\n",
    "\n",
    "    # Train the model\n",
    "    grid_search[sheet].fit(dfs_X_train[sheet], dfs_Y_train[sheet])\n",
    "\n",
    "    # Print the best parameters    \n",
    "    print(f\"Best parameters for {sheet}: \", grid_search[sheet].best_params_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "441f72b3",
   "metadata": {},
   "source": [
    "Notice two parameters that we pass to the `GridSearchCV` function: `cv` and `n_jobs`.  \n",
    "\n",
    "The `cv` parameter in `GridSearchCV` determines the number of folds to use in **k-fold cross-validation**. K-fold cross-validation is a technique used for assessing how well a machine learning model will generalize to an independent data set.\n",
    "\n",
    "In k-fold cross-validation, the training data is randomly partitioned into k equal sized subsamples. Of the k subsamples, a single subsample is retained as the validation data for testing the model, and the remaining k − 1 subsamples are used as training data. The cross-validation process is then repeated k times, with each of the k subsamples used exactly once as the validation data. The k results can then be averaged to produce a single estimation.\n",
    "\n",
    "The `n_jobs` defines how many CPU cores to use for the training. Setting the value to -1 will use all available CPU cores, dramatically shortening the overall time required for the training."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "824caee7",
   "metadata": {},
   "source": [
    "### **Evaluating the models accuracy**\n",
    "\n",
    "Let's see how well the models perform on the test set. we will use the `mean_squared_error` function from scikit-learn to calculate the Mean Squared Error (MSE) between the actual score and the predicted score for the test set. The MSE is a measure of how close the predicted score is to the actual score. The lower the MSE, the more accurate the model is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a6111e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "test_error = {}\n",
    "\n",
    "for sheet in sheets:\n",
    "    # Use the best estimator to make predictions on the test set\n",
    "    Y_test_pred = grid_search[sheet].predict(dfs_X_test[sheet])\n",
    "\n",
    "    # Calculate the test set error\n",
    "    test_error[sheet] = mean_squared_error(dfs_Y_test[sheet], Y_test_pred)\n",
    "\n",
    "    print(f\"Test MSE for {sheet}: \", test_error[sheet])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b76e429b",
   "metadata": {},
   "source": [
    "### **Interpreting the test results**\n",
    "\n",
    "As we are relying on the `GridSearchCV` function to validate the model for us, we will only need to jumpt straight to testing the model against the test set. If the test results are satisfactory, we can finalize the model and use it for production.\n",
    "\n",
    "The values you'll see above is the MSE for the score prediction. For instance, if the MSE is 1.5, it means that the predicted score is off by 1.5 from the actual score (in average).\n",
    "\n",
    "#### So to conclude: \n",
    "\n",
    "If the MSE is too high (and it's up to you to decide what is considered \"too high\"), it means that the model is not accurate enough. We can try to improve the model by:\n",
    "\n",
    "* Adding more training data\n",
    "* Improve the training data quality\n",
    "* Modify the hyperparameters\n",
    "* Adding more features\n",
    "* Using a different algorithm\n",
    "\n",
    "**This is where the iterative process of building and improving the model comes into play!**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3628b96f",
   "metadata": {},
   "source": [
    "### **Storing the model for future use**\n",
    "\n",
    "The models that we've trained are currently stored in memory only.\n",
    "We would like to persist the models in separate files, so that we'll be able to load those later in order to predict the score for new data.\n",
    "\n",
    "For this task, we'll use the `dump` function of the popular `pickle` Python library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a4cb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "for sheet in sheets:\n",
    "    \n",
    "    with open(f'models\\\\{sheet.lower()}_model.pkl', 'wb') as f:\n",
    "        pickle.dump(grid_search[sheet], f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7bd5ad",
   "metadata": {},
   "source": [
    "### **Final Notes**\n",
    "\n",
    "We have now trained the models, evaluated their accuracy using test sets, and stored them in separate files. We can now use the models to predict the score for new data. In case you want to re-train the models, you can simply run the notebook again. The model files will be overwritten with the new models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
