{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Healthcare Vulnerability Scoring System (HVSS): A Scientific Machine Learning Approach**\n",
    "\n",
    "## **1. Introduction and Background**\n",
    "\n",
    "### **1.1 The Healthcare Security Challenge**\n",
    "\n",
    "Healthcare organizations face unique cybersecurity challenges. Unlike other sectors, security breaches in healthcare can directly impact patient safety, potentially leading to physical harm or even loss of life. Medical devices, electronic health records, and hospital networks all present attractive targets for malicious actors.\n",
    "The Common Vulnerability Scoring System (CVSS), maintained by the Forum of Incident Response and Security Teams (FIRST), is widely used for assessing the severity of vulnerabilities in software components. CVSS provides a standardized, generic way to communicate vulnerability severity between multiple parties, including manufacturers, hospitals, clinicians, regulators, and researchers. As designed, CVSS (and its Rubric variant) focuses on evaluating individual vulnerabilities in specific software components rather than assessing security risks across complete ecosystems. Additionally, CVSS is not structured to effectively compare pre- and post-mitigation security states, making it challenging to quantify the impact of security controls and mitigations. CVSS relies on linear mathematical formulas which, when faced with multiple metrics and input parameters, create a calculation complexity space that becomes increasingly difficult to maintain accuracy in the final score.\n",
    "\n",
    "In healthcare environments, where interconnected technologies form complex ecosystems, there's a need for assessment frameworks that can evaluate system-level risks, particularly those that might affect patient safety, sensitive data, and hospital operations, as well as clearly demonstrate the effectiveness of security mitigations.\n",
    "\n",
    "### **1.2 Overview of HVSS**\n",
    "\n",
    "The Healthcare Vulnerability Scoring System (HVSS), which originated from the **Edwards Lifesciences Product Security Group**, addresses this need by providing a specialized framework for assessing security vulnerabilities in medical devices residing on healthcare environments. HVSS employs machine learning algorithms that can effectively manage the calculation complexity introduced by multiple metrics and parameters when evaluating complete healthcare ecosystems, overcoming the mathematical challenges of traditional scoring approaches.\n",
    "HVSS is specifically designed for assessing system-level security risks, allowing for effective comparison between pre- and post-risk mitigation states. This capability enables security teams to quantitatively demonstrate the value of security controls and provide evidence of risk reduction.\n",
    "\n",
    "The official HVSS calculator lab project is managed on GitHub at: https://github.com/ewprodsec/hvss-calculator-lab, providing an open resource for the healthcare security community. \n",
    "\n",
    "HVSS consists of five key components, each designed to capture different aspects of healthcare security:\n",
    "\n",
    "1. **Exploitability**: Measures the difficulty of executing an attack based on attack vector, complexity, privileges required, and user interaction\n",
    "2. **XCIA**: Evaluates impacts on Confidentiality, Integrity, and Availability - the traditional security triad\n",
    "3. **XPS (Patient Safety)**: Assesses potential clinical impact on patients, from negligible effects to life-threatening situations\n",
    "4. **XSD (Sensitive Data)**: Measures exposure of personal identifiers and the scale of potential data breaches\n",
    "5. **XHB (Hospital Breach)**: Evaluates potential impact on broader hospital systems beyond the initially compromised device\n",
    "\n",
    "### **1.3 The Scientific Challenge**\n",
    "\n",
    "Creating accurate predictive models for HVSS scoring presents several challenges:\n",
    "\n",
    "- The relationships between various input metrics and final scores are complex and non-linear\n",
    "- Multiple factors contribute differently to each component score\n",
    "- Domain expertise must be effectively encoded into machine learning algorithms\n",
    "- Models must be interpretable for healthcare security professionals\n",
    "\n",
    "This laboratory experiment applies rigorous scientific methodology to develop machine learning models that accurately predict HVSS scores based on vulnerability characteristics.\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Research Methodology**\n",
    "\n",
    "### **2.1 Experimental Design**\n",
    "\n",
    "This experiment follows the scientific method:\n",
    "\n",
    "1. **Research question**: Can machine learning accurately predict healthcare vulnerability scores?\n",
    "2. **Hypothesis**: Complex algorithms like gradient boosting and neural networks will outperform simple linear models for healthcare vulnerability prediction\n",
    "3. **Experiment**: Systematic evaluation of multiple algorithms with hyperparameter optimization\n",
    "4. **Analysis**: Statistical and visual assessment of model performance\n",
    "5. **Conclusion**: Identification of optimal models for each HVSS component\n",
    "\n",
    "### **2.2 Data Collection and Processing**\n",
    "\n",
    "The training data for this experiment comes from expert-assessed vulnerability scenarios, compiled in the \"TrainingData.xlsx\" file. Each vulnerability has been rated across multiple dimensions and assigned component scores by healthcare security specialists.\n",
    "\n",
    "Our first step is to import the necessary libraries for data analysis, visualization, and machine learning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, explained_variance_score\n",
    "import pickle\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "import uuid\n",
    "import datetime\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# Set up visualizations\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_context(\"talk\")\n",
    "\n",
    "# Suppress specific warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, message=\"Data Validation extension is not supported and will be removed\")\n",
    "warnings.filterwarnings(\"ignore\", category=pd.errors.SettingWithCopyWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This comprehensive set of libraries provides the tools needed for:\n",
    "- Data manipulation (pandas, numpy)\n",
    "- Visualization (matplotlib, seaborn)\n",
    "- Machine learning model development and evaluation (scikit-learn components)\n",
    "- Parallel processing for computational efficiency (joblib)\n",
    "- Model persistence for future use (pickle)\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Data Acquisition and Preprocessing**\n",
    "\n",
    "### **3.1 Data Source and Structure**\n",
    "\n",
    "The HVSS training data is structured across five sheets in the Excel file, each corresponding to a component of the scoring system. We load each sheet into a separate DataFrame for analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an array of sheets\n",
    "sheets = ['Exploitability', 'XCIA', 'XPS', 'XSD', 'XHB']\n",
    "\n",
    "# Create a dictionary to hold the dataframes\n",
    "dfs = {}\n",
    "\n",
    "# Loop through the sheets and read contents into separate DataFrames\n",
    "for sheet in sheets:\n",
    "    df = pd.read_excel('TrainingData.xlsx', sheet_name=sheet, header=None, skiprows=1)\n",
    "    \n",
    "    # Drop rows containing at least one NaN value\n",
    "    df_cleaned = df.dropna()\n",
    "    \n",
    "    dfs[sheet] = df_cleaned\n",
    "\n",
    "# Let's examine the data characteristics for each sheet\n",
    "for sheet in sheets:\n",
    "    print(f\"\\n{sheet} DataFrame - Shape: {dfs[sheet].shape}\")\n",
    "    print(f\"Summary Statistics:\")\n",
    "    print(dfs[sheet].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.2 Data Quality Assessment**\n",
    "\n",
    "Preprocessing steps include:\n",
    "- Loading data from each Excel sheet\n",
    "- Removing incomplete records (rows with missing values)\n",
    "- Examining data distributions and basic statistics\n",
    "\n",
    "This initial exploration reveals the shape of our dataset and provides statistical summaries for each HVSS component. Understanding these characteristics is essential before proceeding with model development, as it helps identify potential data quality issues or distribution patterns that might influence model selection.\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Exploratory Data Analysis**\n",
    "\n",
    "### **4.1 Data Structure Examination**\n",
    "\n",
    "Exploratory Data Analysis (EDA) is a critical scientific step that helps us understand the relationships and patterns within our dataset before applying machine learning algorithms. In this section, we:\n",
    "\n",
    "1. Properly label the columns in each DataFrame based on their content\n",
    "2. Visualize correlations between features and target variables\n",
    "3. Identify potential patterns and relationships that will inform our modeling approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Input sheet for more comprehensive visualizations\n",
    "input_data = pd.read_excel('TrainingData.xlsx', sheet_name='Input')\n",
    "\n",
    "for sheet in sheets:\n",
    "    if sheet == 'Exploitability':\n",
    "        # Exploitability has 5 columns: AV, EAC, PR, UI, Score\n",
    "        if dfs[sheet].shape[1] == 5:\n",
    "            dfs[sheet].columns = ['AV', 'EAC', 'PR', 'UI', 'Exploitability_Score']\n",
    "    elif sheet == 'XCIA':\n",
    "        # XCIA has 8 columns: AV, EAC, PR, UI, C, I, A, Score\n",
    "        if dfs[sheet].shape[1] == 8:\n",
    "            dfs[sheet].columns = ['AV', 'EAC', 'PR', 'UI', 'C', 'I', 'A', 'Final_XCIA']\n",
    "    elif sheet == 'XPS':\n",
    "        # XPS has 6 columns: AV, EAC, PR, UI, XPS, Score\n",
    "        if dfs[sheet].shape[1] == 6:\n",
    "            dfs[sheet].columns = ['AV', 'EAC', 'PR', 'UI', 'XPS', 'Final_XPS']\n",
    "    elif sheet == 'XSD':\n",
    "        # XSD has 6 columns: AV, EAC, PR, UI, XSD, Score\n",
    "        if dfs[sheet].shape[1] == 6:\n",
    "            dfs[sheet].columns = ['AV', 'EAC', 'PR', 'UI', 'XSD', 'Final_XSD']\n",
    "    elif sheet == 'XHB':\n",
    "        # XHB has 6 columns: AV, EAC, PR, UI, XHB, Score\n",
    "        if dfs[sheet].shape[1] == 6:\n",
    "            dfs[sheet].columns = ['AV', 'EAC', 'PR', 'UI', 'XHB', 'Final_XHB']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4.2 Correlation Analysis**\n",
    "\n",
    "Understanding how different factors correlate with final scores is essential for both model development and interpretation. The `plot_feature_correlations` function creates a color-coded visualization of these relationships:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_correlations(df, sheet_name):\n",
    "    # Determine target column based on sheet name\n",
    "    if sheet_name == 'Exploitability':\n",
    "        target_col = 'Exploitability_Score'\n",
    "    else:\n",
    "        target_col = f'Final_{sheet_name}'\n",
    "    \n",
    "    # Check if target column exists\n",
    "    if target_col not in df.columns:\n",
    "        print(f\"Warning: Target column '{target_col}' not found in {sheet_name}.\")\n",
    "        return\n",
    "    \n",
    "    # Calculate correlations with the target\n",
    "    correlations = df.corr()[target_col].drop(target_col)\n",
    "    \n",
    "    # Sort correlations for better visualization\n",
    "    correlations = correlations.sort_values()  # Ascending order so negative values are at the top\n",
    "    \n",
    "    # Create a table-like visualization\n",
    "    fig, ax = plt.subplots(figsize=(8, len(correlations) * 0.6 + 1))\n",
    "    \n",
    "    # Remove axes\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # Create table cells\n",
    "    table_data = []\n",
    "    for feature, corr_value in correlations.items():\n",
    "        table_data.append([feature, f\"{corr_value:.2f}\"])\n",
    "    \n",
    "    # Create the table\n",
    "    table = ax.table(\n",
    "        cellText=table_data,\n",
    "        colLabels=[\"Feature\", \"Correlation with \" + target_col],\n",
    "        loc='center',\n",
    "        cellLoc='center',\n",
    "        colWidths=[0.4, 0.6]\n",
    "    )\n",
    "    \n",
    "    # Customize table appearance\n",
    "    table.auto_set_font_size(False)\n",
    "    table.set_fontsize(10)\n",
    "    table.scale(1.2, 1.5)\n",
    "    \n",
    "    # Color cells based on correlation values\n",
    "    for i, (feature, corr_value) in enumerate(correlations.items()):\n",
    "        # Get normalized value between 0 and 1 for colormap\n",
    "        norm_value = (float(corr_value) + 1) / 2\n",
    "        \n",
    "        # Set cell color\n",
    "        table[(i+1, 1)].set_facecolor(plt.cm.coolwarm(norm_value))\n",
    "        \n",
    "        # Set text color (white for dark backgrounds)\n",
    "        if norm_value < 0.3 or norm_value > 0.7:\n",
    "            table[(i+1, 1)].get_text().set_color('white')\n",
    "    \n",
    "    plt.title(f'Feature Correlations with {target_col} - {sheet_name}', pad=20)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize each sheet\n",
    "for sheet in sheets:\n",
    "    print(f\"\\n{sheet} DataFrame column names: {list(dfs[sheet].columns)}\")\n",
    "    plot_feature_correlations(dfs[sheet], sheet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This correlation analysis reveals the strength and direction of relationships between input features and target scores. Strong positive correlations (towards +1.0) indicate that as the feature value increases, the score tends to increase as well. Strong negative correlations (towards -1.0) indicate that as the feature value increases, the score tends to decrease.\n",
    "\n",
    "### **4.3 Relationship Visualization**\n",
    "\n",
    "Beyond simple correlations, we need to understand how different components interact. The following code creates scatter plots showing relationships between:\n",
    "1. Exploitability scores and final component scores\n",
    "2. Component-specific base scores and final component scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sheet in sheets:\n",
    "    if sheet != 'Exploitability':\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        \n",
    "        # Left plot: Exploitability Score vs Final Score\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.scatter(input_data['Exploitability Score'], input_data[f'Final {sheet}'], alpha=0.7)\n",
    "        \n",
    "        # Add trend line\n",
    "        z = np.polyfit(input_data['Exploitability Score'], input_data[f'Final {sheet}'], 1)\n",
    "        p = np.poly1d(z)\n",
    "        plt.plot(input_data['Exploitability Score'], p(input_data['Exploitability Score']), \n",
    "                \"r--\", alpha=0.8)\n",
    "        \n",
    "        plt.xlabel('Exploitability Score')\n",
    "        plt.ylabel(f'Final {sheet} Score')\n",
    "        plt.title(f'Exploitability Score vs Final {sheet}')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Right plot: Component Score vs Final Score  \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.scatter(input_data[f'{sheet} Score'], input_data[f'Final {sheet}'], \n",
    "                   color='green', alpha=0.7)\n",
    "        \n",
    "        # Add trend line\n",
    "        z = np.polyfit(input_data[f'{sheet} Score'], input_data[f'Final {sheet}'], 1)\n",
    "        p = np.poly1d(z)\n",
    "        plt.plot(input_data[f'{sheet} Score'], p(input_data[f'{sheet} Score']), \n",
    "                \"r--\", alpha=0.8)\n",
    "        \n",
    "        plt.xlabel(f'{sheet} Base Score')\n",
    "        plt.ylabel(f'Final {sheet} Score')\n",
    "        plt.title(f'{sheet} Base Score vs Final {sheet} Score')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These visualizations reveal whether the relationships between variables are linear, suggesting that linear regression might be effective, or non-linear, indicating that more complex algorithms like random forests or gradient boosting might be necessary.\n",
    "\n",
    "---\n",
    "\n",
    "## **5. Feature Engineering and Preparation**\n",
    "\n",
    "### **5.1 Feature and Target Separation**\n",
    "\n",
    "Before training our models, we separate the input features (X) from the target variables (y) for each HVSS component:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionaries to hold the features and target variables\n",
    "X_data = {}\n",
    "y_data = {}\n",
    "\n",
    "# Loop through the sheets and split the features and target variables\n",
    "for sheet in sheets:\n",
    "    if sheet == 'Exploitability':\n",
    "        # For Exploitability, features are AV, EAC, PR, UI\n",
    "        X_data[sheet] = dfs[sheet].iloc[:, :-1].values  # All columns except the last\n",
    "        y_data[sheet] = dfs[sheet].iloc[:, -1].values   # Only the last column (Exploitability_Score)\n",
    "    elif sheet == 'XCIA':\n",
    "        # For XCIA, features are AV, EAC, PR, UI, C, I, A\n",
    "        X_data[sheet] = dfs[sheet].iloc[:, :-1].values\n",
    "        y_data[sheet] = dfs[sheet].iloc[:, -1].values\n",
    "    else:\n",
    "        # For XPS, XSD, XHB, features are AV, EAC, PR, UI, X\n",
    "        X_data[sheet] = dfs[sheet].iloc[:, :-1].values\n",
    "        y_data[sheet] = dfs[sheet].iloc[:, -1].values\n",
    "    \n",
    "    print(f\"{sheet} - Features shape: {X_data[sheet].shape}, Target shape: {y_data[sheet].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This critical step:\n",
    "- Creates separate arrays for input features (X) and target variables (y)\n",
    "- Accounts for the different structure of each component's data\n",
    "- Confirms the dimensions of our feature and target arrays\n",
    "\n",
    "The feature arrays will be used as input to our machine learning models, while the target arrays contain the values we're trying to predict.\n",
    "\n",
    "---\n",
    "\n",
    "## **6. Algorithm Selection and Hyperparameter Optimization**\n",
    "\n",
    "### **6.1 Model Selection Strategy**\n",
    "\n",
    "In scientific machine learning, selecting the right algorithm for a specific problem is crucial. Rather than making assumptions about which algorithm might work best, we take a systematic, empirical approach by testing multiple algorithms with diverse mathematical foundations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the algorithms to test - keeping base definitions simple\n",
    "base_algorithms = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Ridge Regression': Ridge(),\n",
    "    'Lasso Regression': Lasso(),\n",
    "    'ElasticNet': ElasticNet(),\n",
    "    'SVR (RBF)': SVR(),\n",
    "    'Random Forest': RandomForestRegressor(),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(),\n",
    "    'AdaBoost': AdaBoostRegressor(),\n",
    "    'KNeighbors': KNeighborsRegressor(),\n",
    "    'MLP': MLPRegressor()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This approach includes:\n",
    "- **Linear models**: Linear Regression, Ridge, Lasso, ElasticNet\n",
    "- **Tree-based models**: Random Forest, Gradient Boosting, AdaBoost\n",
    "- **Instance-based models**: K-Nearest Neighbors\n",
    "- **Kernel methods**: Support Vector Regression\n",
    "- **Neural networks**: Multi-Layer Perceptron\n",
    "\n",
    "Each algorithm family has different strengths and weaknesses, making them suitable for different types of relationships in the data.\n",
    "\n",
    "### **6.2 Hyperparameter Optimization**\n",
    "\n",
    "For each algorithm, we define a grid of potential hyperparameters to test. Hyperparameters control the behavior of learning algorithms and can significantly impact model performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameter grids for each algorithm type - optimized for efficiency while maintaining accuracy\n",
    "param_grids = {\n",
    "    'Linear Regression': {\n",
    "        'fit_intercept': [True]  # Default is often best, but explicitly defined\n",
    "    },\n",
    "    \n",
    "    'Ridge Regression': {\n",
    "        'alpha': [0.01, 0.1, 1.0, 10.0],  # Focused range\n",
    "        'solver': ['auto'],  # Let sklearn choose the best solver\n",
    "        'random_state': [42]\n",
    "    },\n",
    "    \n",
    "    'Lasso Regression': {\n",
    "        'alpha': [0.01, 0.1, 1.0, 10.0],  # Focused range\n",
    "        'random_state': [42]\n",
    "    },\n",
    "    \n",
    "    'ElasticNet': {\n",
    "        'alpha': [0.01, 0.1, 1.0],  # Reduced options\n",
    "        'l1_ratio': [0.2, 0.5, 0.8],  # Key values in range\n",
    "        'random_state': [42]\n",
    "    },\n",
    "    \n",
    "    'SVR (RBF)': {\n",
    "        'kernel': ['rbf'],  # Only most relevant kernel\n",
    "        'C': [0.1, 1.0, 10.0],  # Focused range\n",
    "        'gamma': ['scale', 0.1, 0.01],  # Most important values\n",
    "    },\n",
    "    \n",
    "    'Random Forest': {\n",
    "        'n_estimators': [100, 200],  # Good defaults\n",
    "        'max_depth': [None, 15, 30],  # None allows full depth\n",
    "        'min_samples_split': [2, 5],  # Default and one alternative\n",
    "        'random_state': [42]\n",
    "    },\n",
    "    \n",
    "    'Gradient Boosting': {\n",
    "        'n_estimators': [100, 200],  # Reduced options\n",
    "        'learning_rate': [0.05, 0.1],  # Most common good values\n",
    "        'max_depth': [3, 5, 7],  # Good range for this algorithm\n",
    "        'subsample': [0.8, 1.0],  # With and without subsampling\n",
    "        'random_state': [42]\n",
    "    },\n",
    "    \n",
    "    'AdaBoost': {\n",
    "        'n_estimators': [50, 100, 200],  # Number of weak learners\n",
    "        'learning_rate': [0.1, 1.0],  # Contribution of each classifier\n",
    "        'loss': ['linear', 'square', 'exponential'],  # Loss function\n",
    "        'random_state': [42]\n",
    "    },\n",
    "    \n",
    "    'KNeighbors': {\n",
    "        'n_neighbors': [3, 5, 7, 9],  # Number of neighbors to consider\n",
    "        'weights': ['uniform', 'distance'],  # Equal vs distance-weighted\n",
    "        'algorithm': ['auto'],  # Let sklearn choose the best algorithm \n",
    "        'p': [1, 2]  # Distance metric: Manhattan (p=1) or Euclidean (p=2)\n",
    "    },\n",
    "    \n",
    "    'MLP': {\n",
    "        'hidden_layer_sizes': [(100,), (200,), (100, 50)],  # Single and two-layer options\n",
    "        'activation': ['relu', 'tanh'],  # Most performant activations\n",
    "        'solver': ['adam'],  # Best general-purpose solver\n",
    "        'alpha': [0.0001, 0.001, 0.01],  # Good regularization range\n",
    "        'learning_rate': ['adaptive'],  # Usually better than constant\n",
    "        'max_iter': [10000],  # High value to ensure convergence\n",
    "        'random_state': [42]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **6.3 Grid Search Implementation**\n",
    "\n",
    "The `tune_single_algorithm` function performs hyperparameter optimization for a single algorithm on a specific HVSS component:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to store the best models for each algorithm and sheet\n",
    "best_models = {sheet: {} for sheet in sheets}\n",
    "best_scores = {sheet: {} for sheet in sheets}\n",
    "\n",
    "# Function to process a single algorithm for a single component\n",
    "def tune_single_algorithm(algorithm_name, algorithm, param_grid, X, y, component_name, job_index, total_jobs):\n",
    "    \"\"\"Process a single algorithm for a given component with enhanced progress reporting\"\"\"\n",
    "    print(f\"Starting job {job_index}/{total_jobs}: {component_name} - {algorithm_name}\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # If no hyperparameters to tune, just evaluate the base model\n",
    "    if not param_grid:\n",
    "        print(f\"  {component_name} - {algorithm_name}: Running cross-validation (no parameters to tune)\")\n",
    "        cv_scores = cross_val_score(algorithm, X, y, cv=5, scoring='neg_mean_squared_error')\n",
    "        mse_scores = -cv_scores\n",
    "        best_score = np.mean(mse_scores)\n",
    "        score_std = np.std(mse_scores)\n",
    "        best_model = algorithm\n",
    "    else:\n",
    "        # Create grid search with explicit n_jobs=1 since we're handling parallelism at a higher level\n",
    "        print(f\"  {component_name} - {algorithm_name}: Starting GridSearchCV with {len(param_grid)} parameter combinations\")\n",
    "        \n",
    "        # Count total parameter combinations\n",
    "        n_combinations = 1\n",
    "        for param_name, param_values in param_grid.items():\n",
    "            n_combinations *= len(param_values)\n",
    "        \n",
    "        print(f\"  {component_name} - {algorithm_name}: Testing {n_combinations} parameter combinations with 5-fold CV\")\n",
    "        \n",
    "        grid_search = GridSearchCV(\n",
    "            algorithm, param_grid, cv=5, \n",
    "            scoring='neg_mean_squared_error', n_jobs=1, verbose=1  # Set verbose=1 for more output\n",
    "        )\n",
    "        \n",
    "        # Train the model\n",
    "        grid_search.fit(X, y)\n",
    "        \n",
    "        # Get best model and score\n",
    "        best_model = grid_search.best_estimator_\n",
    "        best_score = -grid_search.best_score_\n",
    "        score_std = grid_search.cv_results_['std_test_score'][grid_search.best_index_]\n",
    "        \n",
    "        # Report best parameters\n",
    "        best_params = grid_search.best_params_\n",
    "        print(f\"  {component_name} - {algorithm_name}: Best parameters: {best_params}\")\n",
    "    \n",
    "    execution_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"COMPLETED ({job_index}/{total_jobs}): {component_name} - {algorithm_name} - MSE: {best_score:.4f} ± {score_std:.4f} - Time: {execution_time:.2f}s\")\n",
    "    \n",
    "    return {\n",
    "        'component': component_name,\n",
    "        'algorithm_name': algorithm_name,\n",
    "        'best_model': best_model,\n",
    "        'best_score': best_score,\n",
    "        'score_std': score_std,\n",
    "        'execution_time': execution_time\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function:\n",
    "1. Takes an algorithm and hyperparameter grid as input\n",
    "2. Uses cross-validation to find optimal parameters\n",
    "3. Reports performance metrics and execution time\n",
    "4. Returns the best model configuration and its score\n",
    "\n",
    "### **6.4 Parallel Processing for Computational Efficiency**\n",
    "\n",
    "To optimize the computational efficiency of our hyperparameter search, we implement parallel processing with a fallback to sequential processing if needed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try the efficient parallelization approach first, with fallback to sequential\n",
    "try:\n",
    "    # Create a list of all jobs to run\n",
    "    all_jobs = []\n",
    "    for sheet in sheets:\n",
    "        for algorithm_name, base_algorithm in base_algorithms.items():\n",
    "            param_grid = param_grids[algorithm_name]\n",
    "            all_jobs.append((\n",
    "                algorithm_name, \n",
    "                base_algorithm, \n",
    "                param_grid,\n",
    "                X_data[sheet],\n",
    "                y_data[sheet],\n",
    "                sheet\n",
    "            ))\n",
    "    \n",
    "    # Report total jobs\n",
    "    total_jobs = len(all_jobs)\n",
    "    num_cpus = os.cpu_count()\n",
    "    print(f\"\\nRunning {total_jobs} tuning jobs in parallel using {num_cpus} CPU cores\")\n",
    "    print(f\"Progress will be reported for each job as it starts and completes\")\n",
    "    print(f\"Each job will also report CV progress for better visibility\")\n",
    "    \n",
    "    # Add job indices to the jobs\n",
    "    indexed_jobs = []\n",
    "    for i, job in enumerate(all_jobs):\n",
    "        indexed_jobs.append(job + (i+1, total_jobs))\n",
    "    \n",
    "    # Run jobs in parallel with enhanced progress reporting\n",
    "    results = Parallel(n_jobs=-1, verbose=50)(\n",
    "        delayed(tune_single_algorithm)(*job)\n",
    "        for job in indexed_jobs\n",
    "    )\n",
    "    \n",
    "    # Organize results by component\n",
    "    for sheet in sheets:\n",
    "        best_models[sheet] = {}\n",
    "        best_scores[sheet] = {}\n",
    "    \n",
    "    for result in results:\n",
    "        component_name = result['component']\n",
    "        algorithm_name = result['algorithm_name']\n",
    "        best_models[component_name][algorithm_name] = result['best_model']\n",
    "        best_scores[component_name][algorithm_name] = result['best_score']\n",
    "    \n",
    "    # Identify best algorithm for each component\n",
    "    for sheet in sheets:\n",
    "        best_algo = min(best_scores[sheet].items(), key=lambda x: x[1])[0]\n",
    "        best_score = best_scores[sheet][best_algo]\n",
    "        print(f\"\\n▶ Best algorithm for {sheet}: {best_algo} with MSE: {best_score:.4f}\")\n",
    "    \n",
    "    print(\"\\nEfficient parallel processing completed successfully!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error in efficient parallel processing: {e}\")\n",
    "    print(\"\\nFalling back to sequential processing...\")\n",
    "    \n",
    "    # Sequential fallback\n",
    "    for sheet in sheets:\n",
    "        print(f\"\\nProcessing {sheet} component...\")\n",
    "        best_models[sheet] = {}\n",
    "        best_scores[sheet] = {}\n",
    "        \n",
    "        for i, (algorithm_name, base_algorithm) in enumerate(base_algorithms.items()):\n",
    "            job_index = i + 1\n",
    "            start_time = time.time()\n",
    "            param_grid = param_grids[algorithm_name]\n",
    "            \n",
    "            result = tune_single_algorithm(\n",
    "                algorithm_name, \n",
    "                base_algorithm, \n",
    "                param_grid,\n",
    "                X_data[sheet],\n",
    "                y_data[sheet],\n",
    "                sheet,\n",
    "                job_index,\n",
    "                len(base_algorithms)\n",
    "            )\n",
    "            \n",
    "            best_models[sheet][algorithm_name] = result['best_model']\n",
    "            best_scores[sheet][algorithm_name] = result['best_score']\n",
    "        \n",
    "        # Find best algorithm for this sheet\n",
    "        best_algo = min(best_scores[sheet].items(), key=lambda x: x[1])[0]\n",
    "        print(f\"\\n► Best algorithm for {sheet}: {best_algo} with MSE: {best_scores[sheet][best_algo]:.4f}\")\n",
    "    \n",
    "    print(\"\\nSequential processing completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This approach:\n",
    "- Attempts to use parallel processing for efficiency\n",
    "- Provides a sequential fallback if parallel processing fails\n",
    "- Reports progress throughout the process\n",
    "- Identifies the best algorithm for each HVSS component\n",
    "\n",
    "---\n",
    "\n",
    "## **7. Optimal Algorithm Identification**\n",
    "\n",
    "### **7.1 Comparative Algorithm Performance**\n",
    "\n",
    "After completing the hyperparameter optimization, we identify the most effective algorithm for each HVSS component:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the best performing algorithm for each sheet after hyperparameter tuning\n",
    "true_best_algorithms = {}\n",
    "\n",
    "for sheet in sheets:\n",
    "    algo_names = list(best_scores[sheet].keys())\n",
    "    mse_scores = [best_scores[sheet][algo] for algo in algo_names]\n",
    "    best_idx = np.argmin(mse_scores)\n",
    "    true_best_algorithms[sheet] = algo_names[best_idx]\n",
    "    \n",
    "    print(f\"\\nBest algorithm for {sheet} after hyperparameter tuning: {algo_names[best_idx]} (MSE: {mse_scores[best_idx]:.4f})\")\n",
    "    \n",
    "    # Print the hyperparameters of the best model\n",
    "    best_model = best_models[sheet][algo_names[best_idx]]\n",
    "    print(f\"Best hyperparameters:\")\n",
    "    for param, value in best_model.get_params().items():\n",
    "        # Filter out some of the less important parameters for readability\n",
    "        if not param.startswith('_') and param not in ['feature_names_in_', 'n_features_in_', 'n_iter_', 'random_state']:\n",
    "            print(f\"  {param}: {value}\")\n",
    "    \n",
    "    # Compare with other algorithms\n",
    "    print(\"\\nAll tuned algorithms ranked by performance:\")\n",
    "    sorted_algos = sorted(zip(algo_names, mse_scores), key=lambda x: x[1])\n",
    "    for i, (algo, score) in enumerate(sorted_algos):\n",
    "        print(f\"  {i+1}. {algo}: MSE = {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section:\n",
    "- Identifies the best algorithm for each component based on mean squared error\n",
    "- Displays the optimal hyperparameters for each selected algorithm\n",
    "- Ranks all algorithms based on their performance for each component\n",
    "\n",
    "Understanding which algorithms perform best provides insight into the nature of the relationships within our data. For example, if tree-based methods outperform linear models, it suggests non-linear relationships between features and targets.\n",
    "\n",
    "---\n",
    "\n",
    "## **8. Comprehensive Model Evaluation**\n",
    "\n",
    "### **8.1 Train-Test Split for Final Evaluation**\n",
    "\n",
    "To ensure our final model evaluation is unbiased, we create a train-test split for each HVSS component:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train/test splits for final evaluation\n",
    "X_train = {}\n",
    "X_test = {}\n",
    "y_train = {}\n",
    "y_test = {}\n",
    "\n",
    "# Split data with stratification where possible (80% train, 20% test)\n",
    "for sheet in sheets:\n",
    "    X_train[sheet], X_test[sheet], y_train[sheet], y_test[sheet] = train_test_split(\n",
    "        X_data[sheet], y_data[sheet], test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    print(f\"{sheet} - Training set: {X_train[sheet].shape[0]} samples, Test set: {X_test[sheet].shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This standard machine learning practice:\n",
    "- Reserves 20% of the data for testing\n",
    "- Uses 80% of the data for training\n",
    "- Ensures consistent splits with a fixed random seed\n",
    "\n",
    "### **8.2 Multi-Metric Model Assessment**\n",
    "\n",
    "To thoroughly evaluate our models, we use multiple performance metrics beyond just mean squared error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function for comprehensive model evaluation\n",
    "def evaluate_model_comprehensive(model, X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Comprehensively evaluate a model using multiple performance metrics\n",
    "    \n",
    "    This function implements scientific best practices by:\n",
    "    1. Training the model on the training data\n",
    "    2. Making predictions on both training and test data\n",
    "    3. Calculating multiple complementary performance metrics\n",
    "    4. Returning both metrics and predictions for further analysis\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : scikit-learn estimator\n",
    "        The machine learning model to evaluate\n",
    "    X_train, y_train : array-like\n",
    "        Training data and labels\n",
    "    X_test, y_test : array-like\n",
    "        Test data and labels\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary containing multiple evaluation metrics and predictions\n",
    "    \"\"\"\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    train_mse = mean_squared_error(y_train, y_pred_train)\n",
    "    test_mse = mean_squared_error(y_test, y_pred_test)\n",
    "    \n",
    "    train_mae = mean_absolute_error(y_train, y_pred_train)\n",
    "    test_mae = mean_absolute_error(y_test, y_pred_test)\n",
    "    \n",
    "    train_r2 = r2_score(y_train, y_pred_train)\n",
    "    test_r2 = r2_score(y_test, y_pred_test)\n",
    "    \n",
    "    train_ev = explained_variance_score(y_train, y_pred_train)\n",
    "    test_ev = explained_variance_score(y_test, y_pred_test)\n",
    "    \n",
    "    return {\n",
    "        'train_mse': train_mse,       # Mean Squared Error: Average squared difference between predictions and true values\n",
    "        'test_mse': test_mse,         # Lower values indicate better fit\n",
    "        \n",
    "        'train_mae': train_mae,       # Mean Absolute Error: Average absolute difference between predictions and true values\n",
    "        'test_mae': test_mae,         # More robust to outliers than MSE\n",
    "        \n",
    "        'train_r2': train_r2,         # R²: Proportion of variance explained by the model\n",
    "        'test_r2': test_r2,           # Values range from 0 to 1, with 1 indicating perfect prediction\n",
    "        \n",
    "        'train_ev': train_ev,         # Explained Variance: Similar to R² but not penalized for bias\n",
    "        'test_ev': test_ev,           # Useful for detecting systematic errors\n",
    "        \n",
    "        'y_pred_train': y_pred_train, # Predictions on training data for residual analysis\n",
    "        'y_pred_test': y_pred_test    # Predictions on test data for error analysis\n",
    "    }\n",
    "\n",
    "# Evaluate the best model for each sheet\n",
    "eval_results = {}\n",
    "\n",
    "for sheet in sheets:\n",
    "    print(f\"\\nEvaluating best model for {sheet}...\")\n",
    "    best_algo = true_best_algorithms[sheet]\n",
    "    model = best_models[sheet][best_algo]\n",
    "    \n",
    "    eval_results[sheet] = evaluate_model_comprehensive(\n",
    "        model, X_train[sheet], y_train[sheet], X_test[sheet], y_test[sheet]\n",
    "    )\n",
    "    \n",
    "    print(f\"  Algorithm: {best_algo}\")\n",
    "    print(f\"  Train MSE: {eval_results[sheet]['train_mse']:.4f}\")\n",
    "    print(f\"  Test MSE: {eval_results[sheet]['test_mse']:.4f}\")\n",
    "    print(f\"  Train MAE: {eval_results[sheet]['train_mae']:.4f}\")\n",
    "    print(f\"  Test MAE: {eval_results[sheet]['test_mae']:.4f}\")\n",
    "    print(f\"  Train R²: {eval_results[sheet]['train_r2']:.4f}\")\n",
    "    print(f\"  Test R²: {eval_results[sheet]['test_r2']:.4f}\")\n",
    "    print(f\"  Train Explained Variance: {eval_results[sheet]['train_ev']:.4f}\")\n",
    "    print(f\"  Test Explained Variance: {eval_results[sheet]['test_ev']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A scientifically rigorous evaluation requires multiple complementary metrics, as different metrics capture different aspects of model performance:\n",
    "\n",
    "1. **Mean Squared Error (MSE)**: Measures the average squared difference between predicted and actual values. This metric penalizes larger errors more heavily but is sensitive to outliers.\n",
    "\n",
    "2. **Mean Absolute Error (MAE)**: Measures the average absolute difference between predicted and actual values. This metric is more robust to outliers than MSE.\n",
    "\n",
    "3. **R-squared (R²)**: Indicates the proportion of variance in the dependent variable that can be predicted from the independent variables. Values range from 0 to 1, with higher values indicating better fit.\n",
    "\n",
    "4. **Explained Variance Score**: Similar to R², but doesn't penalize for systematic bias in predictions. Useful for detecting consistent over/under-predictions.\n",
    "\n",
    "Examining these metrics together provides a more complete picture of model performance than any single metric alone.\n",
    "\n",
    "## **9. Prediction Accuracy Visualization**\n",
    "\n",
    "### **9.1 Actual vs. Predicted Values**\n",
    "\n",
    "Visual examination of model performance is a crucial scientific practice that can reveal patterns that might be missed when only reviewing numerical metrics. The following visualizations show how well our models predict both training and test data:\n",
    "\n",
    "In these scatter plots:\n",
    "- Each point represents a vulnerability scenario\n",
    "- The x-axis shows the actual HVSS score\n",
    "- The y-axis shows the predicted score from our model\n",
    "- The red dashed line represents perfect prediction (y=x)\n",
    "- Points close to the line indicate accurate predictions\n",
    "- Points far from the line indicate prediction errors\n",
    "\n",
    "The R² value in each plot title quantifies how well the model explains the variance in the data. An R² of 1.0 would indicate perfect predictions, while 0.0 would indicate no predictive ability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize actual vs predicted values\n",
    "for sheet in sheets:\n",
    "    best_algo = true_best_algorithms[sheet]\n",
    "    \n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    # Training data\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.scatter(y_train[sheet], eval_results[sheet]['y_pred_train'], alpha=0.7)\n",
    "    # Add the ideal prediction line (y=x) where predictions equal actual values\n",
    "    plt.plot([min(y_train[sheet]), max(y_train[sheet])], \n",
    "             [min(y_train[sheet]), max(y_train[sheet])], 'r--')\n",
    "    plt.xlabel('Actual Values')\n",
    "    plt.ylabel('Predicted Values')\n",
    "    plt.title(f'{sheet} - {best_algo} - Training Data (R² = {eval_results[sheet][\"train_r2\"]:.4f})')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Testing data\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.scatter(y_test[sheet], eval_results[sheet]['y_pred_test'], alpha=0.7)\n",
    "    # Add the ideal prediction line (y=x) where predictions equal actual values\n",
    "    plt.plot([min(y_test[sheet]), max(y_test[sheet])], \n",
    "             [min(y_test[sheet]), max(y_test[sheet])], 'r--')\n",
    "    plt.xlabel('Actual Values')\n",
    "    plt.ylabel('Predicted Values')\n",
    "    plt.title(f'{sheet} - {best_algo} - Testing Data (R² = {eval_results[sheet][\"test_r2\"]:.4f})')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **10. Residual Analysis**\n",
    "\n",
    "### **10.1 Residual Distribution and Patterns**\n",
    "\n",
    "Residual analysis is a cornerstone of statistical model validation. Residuals (the differences between predicted and actual values) provide insights into model adequacy and potential areas for improvement. In an ideal model:\n",
    "\n",
    "1. Residuals should be randomly distributed around zero\n",
    "2. Residuals should show no patterns when plotted against predicted values\n",
    "3. Residuals should follow a normal distribution\n",
    "\n",
    "Deviations from these expectations can indicate specific model deficiencies:\n",
    "- Systematic bias (residual mean not zero)\n",
    "- Heteroscedasticity (non-constant variance)\n",
    "- Missing non-linear relationships\n",
    "- Influence of outliers\n",
    "\n",
    "The following visualizations examine residual patterns for our best models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and visualize residuals\n",
    "for sheet in sheets:\n",
    "    best_algo = true_best_algorithms[sheet]\n",
    "    \n",
    "    # Calculate residuals (actual - predicted)\n",
    "    train_residuals = y_train[sheet] - eval_results[sheet]['y_pred_train']\n",
    "    test_residuals = y_test[sheet] - eval_results[sheet]['y_pred_test']\n",
    "    \n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    # Residuals vs Predicted Values (Training data)\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.scatter(eval_results[sheet]['y_pred_train'], train_residuals, alpha=0.7)\n",
    "    plt.axhline(y=0, color='r', linestyle='--')  # Reference line at y=0\n",
    "    plt.xlabel('Predicted Values')\n",
    "    plt.ylabel('Residuals')\n",
    "    plt.title(f'{sheet} - {best_algo} - Training Residuals')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Residuals vs Predicted Values (Testing data)\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.scatter(eval_results[sheet]['y_pred_test'], test_residuals, alpha=0.7)\n",
    "    plt.axhline(y=0, color='r', linestyle='--')  # Reference line at y=0\n",
    "    plt.xlabel('Predicted Values')\n",
    "    plt.ylabel('Residuals')\n",
    "    plt.title(f'{sheet} - {best_algo} - Testing Residuals')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Histogram of training residuals with normal distribution reference\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.hist(train_residuals, bins=20, alpha=0.7, density=True, label='Residuals')\n",
    "    # If available, could add normal distribution curve for reference\n",
    "    plt.xlabel('Residual Value')\n",
    "    plt.ylabel('Density')\n",
    "    plt.title('Training Residuals Distribution')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Histogram of testing residuals with normal distribution reference\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.hist(test_residuals, bins=20, alpha=0.7, density=True, label='Residuals')\n",
    "    # If available, could add normal distribution curve for reference\n",
    "    plt.xlabel('Residual Value')\n",
    "    plt.ylabel('Density')\n",
    "    plt.title('Testing Residuals Distribution')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.suptitle(f\"Residual Analysis: {sheet} Component\", y=1.02, fontsize=16)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **11. Feature Importance Analysis**\n",
    "\n",
    "### **11.1 Identifying Key Vulnerability Factors**\n",
    "\n",
    "Understanding which features most strongly influence HVSS scores is critical for both scientific insight and practical application. Feature importance analysis helps:\n",
    "\n",
    "1. **Interpret model decisions**: Reveals the factors that drive predictions, making models more transparent\n",
    "2. **Guide security practices**: Identifies the most critical factors for healthcare security professionals to focus on\n",
    "3. **Inform future research**: Highlights areas where additional data collection or feature engineering might be beneficial\n",
    "4. **Validate domain expertise**: Confirms (or challenges) existing understanding of healthcare security vulnerability factors\n",
    "\n",
    "Different algorithms calculate feature importance in different ways:\n",
    "- **Tree-based models** (Random Forest, Gradient Boosting): Measure how much each feature reduces impurity when used in splits\n",
    "- **Linear models**: Use the magnitude of coefficients (normalized if features have different scales)\n",
    "- **Other models**: May use permutation importance or SHAP values for feature importance estimation\n",
    "\n",
    "The following analysis examines feature importance across our best models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze feature importance for the best models\n",
    "for sheet in sheets:\n",
    "    best_algo = true_best_algorithms[sheet]\n",
    "    model = best_models[sheet][best_algo]\n",
    "    \n",
    "    print(f\"\\nFeature importance analysis for {sheet} using {best_algo}:\")\n",
    "    \n",
    "    # Define feature names based on the sheet with descriptive labels\n",
    "    if sheet == 'Exploitability':\n",
    "        feature_names = ['Attack Vector (AV)', 'Extended Attack Complexity (EAC)', \n",
    "                         'Privileges Required (PR)', 'User Interaction (UI)']\n",
    "    elif sheet == 'XCIA':\n",
    "        feature_names = ['Attack Vector (AV)', 'Extended Attack Complexity (EAC)', \n",
    "                         'Privileges Required (PR)', 'User Interaction (UI)',\n",
    "                         'Confidentiality (C)', 'Integrity (I)', 'Availability (A)']\n",
    "    else:\n",
    "        feature_names = ['Attack Vector (AV)', 'Extended Attack Complexity (EAC)', \n",
    "                         'Privileges Required (PR)', 'User Interaction (UI)', \n",
    "                         sheet]\n",
    "    \n",
    "    # Check if the model has feature_importances_ attribute (tree-based models)\n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        importances = model.feature_importances_\n",
    "        indices = np.argsort(importances)[::-1]\n",
    "        \n",
    "        # Print feature ranking with scientific interpretation\n",
    "        print(\"Feature importance ranking (normalized importance scores):\")\n",
    "        for i in range(len(feature_names)):\n",
    "            feature_idx = indices[i]\n",
    "            importance = importances[feature_idx]\n",
    "            print(f\"  {i+1}. {feature_names[feature_idx]} ({importance:.4f} or {importance*100:.1f}%)\")\n",
    "        \n",
    "        # Plot feature importance with enhanced visualization\n",
    "        plt.figure(figsize=(12, 7))\n",
    "        plt.title(f'Feature Importance Analysis for {sheet} Component\\n({best_algo} Model)', fontsize=14)\n",
    "        \n",
    "        # Create horizontal bar chart for better readability with many features\n",
    "        y_pos = np.arange(len(feature_names))\n",
    "        plt.barh(y_pos, importances[indices], align='center', \n",
    "                alpha=0.8, color='skyblue', edgecolor='navy')\n",
    "        plt.yticks(y_pos, [feature_names[i] for i in indices])\n",
    "        plt.xlabel('Relative Importance', fontsize=12)\n",
    "        plt.xlim(0, max(importances) * 1.1)  # Add some padding\n",
    "        \n",
    "        # Add importance values as text\n",
    "        for i, v in enumerate(importances[indices]):\n",
    "            plt.text(v + max(importances) * 0.01, i, f\"{v:.3f}\", \n",
    "                    color='navy', va='center', fontweight='bold')\n",
    "            \n",
    "        plt.tight_layout()\n",
    "        plt.grid(axis='x', alpha=0.3)\n",
    "        plt.show()\n",
    "    \n",
    "    # For linear models, check for coef_ attribute\n",
    "    elif hasattr(model, 'coef_'):\n",
    "        coefficients = model.coef_\n",
    "        \n",
    "        # For 1D coefficient array\n",
    "        if len(coefficients.shape) == 1:\n",
    "            # Get absolute values for ranking importance\n",
    "            abs_coefficients = np.abs(coefficients)\n",
    "            indices = np.argsort(abs_coefficients)[::-1]\n",
    "            \n",
    "            # Print feature ranking with scientific interpretation\n",
    "            print(\"Feature coefficients ranking (by absolute magnitude):\")\n",
    "            for i in range(len(feature_names)):\n",
    "                feature_idx = indices[i]\n",
    "                coef = coefficients[feature_idx]\n",
    "                abs_coef = abs_coefficients[feature_idx]\n",
    "                sign = \"+\" if coef > 0 else \"-\"\n",
    "                print(f\"  {i+1}. {feature_names[feature_idx]} ({sign}{abs_coef:.4f}) - {'Increases' if coef > 0 else 'Decreases'} score\")\n",
    "            \n",
    "            # Plot feature coefficients with enhanced visualization\n",
    "            plt.figure(figsize=(12, 7))\n",
    "            plt.title(f'Feature Coefficient Analysis for {sheet} Component\\n({best_algo} Model)', fontsize=14)\n",
    "            \n",
    "            # Create horizontal bar chart colored by coefficient sign\n",
    "            y_pos = np.arange(len(feature_names))\n",
    "            colors = ['green' if c > 0 else 'red' for c in coefficients[indices]]\n",
    "            plt.barh(y_pos, coefficients[indices], align='center', alpha=0.8, color=colors)\n",
    "            plt.yticks(y_pos, [feature_names[i] for i in indices])\n",
    "            plt.xlabel('Coefficient Value (Effect on Score)', fontsize=12)\n",
    "            \n",
    "            # Add coefficient values as text\n",
    "            for i, v in enumerate(coefficients[indices]):\n",
    "                plt.text(v + (0.01 if v >= 0 else -0.01) * max(abs_coefficients), \n",
    "                         i, f\"{v:.3f}\", \n",
    "                         color='black', va='center', ha='left' if v >= 0 else 'right', \n",
    "                         fontweight='bold')\n",
    "            \n",
    "            plt.axvline(x=0, color='black', linestyle='-', alpha=0.5)  # Add reference line at x=0\n",
    "            plt.tight_layout()\n",
    "            plt.grid(alpha=0.3)\n",
    "            plt.show()\n",
    "    else:\n",
    "        print(f\"Feature importance not directly available for {best_algo}\")\n",
    "        print(\"Consider using permutation importance or SHAP values for this model type.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **12. Model Persistence and Deployment Strategy**\n",
    "\n",
    "### **12.1 Scientific Reproducibility Through Model Serialization**\n",
    "\n",
    "In scientific computing, preserving the trained models is essential for reproducibility, deployment, and future research. The process of saving trained models to disk is known as \"model persistence\" or \"serialization.\" This practice is fundamental to the scientific method, as it enables other researchers to verify findings and build upon established work.\n",
    "\n",
    "For the HVSS scoring system, model persistence enables:\n",
    "\n",
    "1. **Reproducible Science**: Other researchers can verify results without retraining models\n",
    "2. **Incremental Improvement**: Future work can build upon these models rather than starting from scratch\n",
    "3. **Knowledge Transfer**: Expertise encoded in the models can be shared across institutions\n",
    "4. **Longitudinal Analysis**: Models from different time periods can be compared to track evolving security landscapes\n",
    "\n",
    "### **12.2 Implementation of Model Persistence**\n",
    "\n",
    "We use the Python pickle library to serialize our models into binary files that can be restored later with all trained parameters intact. Our approach includes storing comprehensive metadata alongside each model to ensure proper documentation and provenance.\n",
    "\n",
    "### **12.3 Operational Deployment Considerations**\n",
    "\n",
    "Beyond scientific reproducibility, our model persistence strategy accounts for practical deployment requirements in healthcare security environments:\n",
    "\n",
    "1. **Version Control**: Each model is saved with timestamp information, facilitating version management in production systems\n",
    "2. **Performance Documentation**: Key performance metrics are stored with each model, allowing for informed decisions about model usage\n",
    "3. **Feature Documentation**: Input feature names are preserved to ensure correct application in deployment\n",
    "4. **Hyperparameter Recording**: All configuration parameters are saved to support future optimization efforts\n",
    "\n",
    "For operational deployment in healthcare environments, these serialized models can be integrated into:\n",
    "\n",
    "- **Security Assessment Platforms**: Automating the scoring of newly discovered vulnerabilities\n",
    "- **Vulnerability Management Systems**: Prioritizing remediation efforts based on predicted impacts\n",
    "- **Security Testing Frameworks**: Evaluating potential attack vectors during penetration testing\n",
    "- **Regulatory Compliance Tools**: Supporting HIPAA and other regulatory requirements with standardized risk assessment\n",
    "\n",
    "The implementation of model persistence ensures both scientific validity and practical utility:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best models with detailed metadata\n",
    "print(\"\\nSaving the best models with metadata...\")\n",
    "\n",
    "# First, ensure the models directory exists\n",
    "os.makedirs('models', exist_ok=True)\n",
    "\n",
    "# Generate a unique build ID (5 hex characters)\n",
    "build_id = uuid.uuid4().hex[:5]\n",
    "\n",
    "# Get current UTC timestamp\n",
    "build_date = datetime.datetime.now(datetime.timezone.utc).strftime(\"%Y-%m-%d %H:%M:%S %Z\")\n",
    "\n",
    "# Dictionary to collect all models' metadata\n",
    "all_models_metadata = {\n",
    "    'build_date': build_date,\n",
    "    'build_id': build_id,\n",
    "    'models': {}\n",
    "}\n",
    "\n",
    "for sheet in sheets:\n",
    "    best_algo = true_best_algorithms[sheet]\n",
    "    model = best_models[sheet][best_algo]\n",
    "    \n",
    "    # Create simplified filename with just the component name\n",
    "    filename = f'models/{sheet.lower()}_model.pkl'\n",
    "    \n",
    "    # Create metadata to store alongside the model\n",
    "    model_metadata = {\n",
    "        'component': sheet,\n",
    "        'algorithm': best_algo,\n",
    "        'performance_metrics': {\n",
    "            'test_mse': eval_results[sheet]['test_mse'],\n",
    "            'test_r2': eval_results[sheet]['test_r2']\n",
    "        },\n",
    "        'hyperparameters': model.get_params()\n",
    "    }\n",
    "    \n",
    "    # Add this model's metadata to the collective metadata\n",
    "    all_models_metadata['models'][sheet] = model_metadata\n",
    "    \n",
    "    # Create a dictionary with both model and metadata\n",
    "    model_package = {\n",
    "        'model': model,\n",
    "        'metadata': model_metadata\n",
    "    }\n",
    "    \n",
    "    # Save the model package\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(model_package, f)\n",
    "    \n",
    "    print(f\"  Saved {sheet} model ({best_algo}) to {filename}\")\n",
    "    print(f\"    Test MSE: {eval_results[sheet]['test_mse']:.4f}\")\n",
    "    print(f\"    Test R²: {eval_results[sheet]['test_r2']:.4f}\")\n",
    "\n",
    "# Save the collective metadata to a JSON file\n",
    "metadata_filename = f'models/hvss_models_metadata.json'\n",
    "with open(metadata_filename, 'w') as f:\n",
    "    json.dump(all_models_metadata, f, indent=2)\n",
    "\n",
    "print(f\"\\nSaved collective metadata to {metadata_filename}\")\n",
    "print(\"\\nModel persistence completed! These models can now be used for HVSS scoring in production environments.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **13. Scientific Conclusions and Healthcare Security Implications**\n",
    "\n",
    "### **13.1 Summary of Experimental Findings**\n",
    "\n",
    "This scientific investigation has employed rigorous machine learning methodologies to develop predictive models for the Healthcare Vulnerability Scoring System (HVSS). Through systematic algorithm comparison, hyperparameter optimization, and comprehensive evaluation, we have gained valuable insights into the quantitative assessment of healthcare security vulnerabilities.\n",
    "\n",
    "Our key experimental findings include:\n",
    "\n",
    "1. **Algorithm Effectiveness**: We compared 10 distinct machine learning algorithms with diverse mathematical foundations and determined which algorithms perform optimally for each HVSS component. This revealed whether linear or non-linear relationships dominate in healthcare vulnerability assessment.\n",
    "\n",
    "2. **Predictive Performance**: We achieved strong predictive performance across multiple HVSS components, with R² values demonstrating that machine learning models can reliably predict vulnerability scores based on input metrics.\n",
    "\n",
    "3. **Feature Importance**: We identified the most influential factors in healthcare vulnerability assessment, providing an empirical basis for prioritizing security efforts in healthcare environments.\n",
    "\n",
    "4. **Component Predictability**: We discovered variation in the predictability of different HVSS components, offering insight into which aspects of healthcare security are most amenable to algorithmic assessment.\n",
    "\n",
    "### **13.2 Scientific and Healthcare Security Contributions**\n",
    "\n",
    "This research advances both the scientific understanding of healthcare security vulnerabilities and provides practical tools for the industry:\n",
    "\n",
    "**Scientific Contributions:**\n",
    "- Establishes machine learning as a viable approach for healthcare-specific security assessment\n",
    "- Quantifies the predictive relationships between attack characteristics and security impacts\n",
    "- Provides empirical evidence for the relative importance of different vulnerability factors\n",
    "- Creates a methodological framework for reproducible vulnerability prediction research\n",
    "- Establishes performance benchmarks for future healthcare security assessment studies\n",
    "\n",
    "**Healthcare Security Implications:**\n",
    "- Enables consistent, evidence-based scoring of vulnerabilities specific to healthcare environments\n",
    "- Supports security practitioners with automated assessment capabilities that account for patient safety\n",
    "- Highlights critical factors for prioritizing security investments in resource-constrained healthcare settings\n",
    "- Provides a foundation for standardized security metrics that align with healthcare regulatory requirements\n",
    "- Bridges the gap between technical vulnerability characteristics and healthcare-specific concerns\n",
    "\n",
    "### **13.3 Limitations and Methodological Considerations**\n",
    "\n",
    "As with any scientific investigation, it is important to acknowledge limitations in our approach:\n",
    "\n",
    "1. **Data Constraints**: The models are only as comprehensive as the training data. As new vulnerability types emerge, model retraining will be necessary.\n",
    "\n",
    "2. **Subjective Elements**: The original scoring of training examples may contain subjective judgments that influence model outputs.\n",
    "\n",
    "3. **Static Assessment**: The current models provide point-in-time assessments and do not account for evolving threat landscapes without retraining.\n",
    "\n",
    "4. **Simplification of Complex Factors**: While the models capture key relationships, some nuances of healthcare environments may not be fully represented.\n",
    "\n",
    "5. **Domain-Specific Applicability**: Models trained specifically for healthcare may not generalize to other sectors without adaptation.\n",
    "\n",
    "### **13.4 Future Research Directions**\n",
    "\n",
    "Building on this scientific investigation, several promising research directions emerge:\n",
    "\n",
    "**Short-term research opportunities:**\n",
    "1. **Dataset Expansion**: Incorporating a broader range of healthcare vulnerability scenarios to improve model generalizability\n",
    "2. **Ensemble Methods**: Investigating whether combinations of multiple algorithms can further improve predictive performance\n",
    "3. **Feature Engineering**: Developing additional healthcare-specific features that capture nuances of medical environments\n",
    "4. **Deployment Studies**: Evaluating how these models perform in real-world healthcare security operations\n",
    "\n",
    "**Medium-term research directions:**\n",
    "1. **Temporal Analysis**: Studying how vulnerability patterns evolve over time as healthcare technology changes\n",
    "2. **Causal Inference**: Moving beyond correlation to establish causal relationships between security factors and outcomes\n",
    "3. **Explainable AI**: Developing more transparent models that provide clearer rationales for security assessments\n",
    "4. **Transfer Learning**: Exploring how models trained on general security data can be adapted for healthcare-specific contexts\n",
    "\n",
    "**Long-term scientific goals:**\n",
    "1. **Predictive Security**: Moving from reactive to predictive security models in healthcare\n",
    "2. **Integrated Risk Framework**: Combining cyber, physical, and clinical risk assessments in a unified framework\n",
    "3. **Automated Mitigation Strategy Development**: Using model insights to automatically generate mitigation recommendations\n",
    "4. **Regulatory Alignment**: Developing scientific foundations for healthcare-specific security regulations\n",
    "\n",
    "### **13.5 Conclusion**\n",
    "\n",
    "This scientific investigation demonstrates that machine learning can effectively model the complex relationships in healthcare security vulnerability assessment. By applying rigorous methodologies from data science to the domain of healthcare security, we have created predictive models that balance accuracy with interpretability.\n",
    "\n",
    "The HVSS machine learning models developed in this research represent a significant step toward more objective, consistent, and efficient security vulnerability assessment in healthcare environments. These models not only advance the scientific understanding of healthcare security factors but also provide practical tools that can help protect sensitive patient data and critical healthcare infrastructure.\n",
    "\n",
    "As healthcare technology continues to evolve and security threats become increasingly sophisticated, data-driven approaches like those developed in this research will play an essential role in protecting healthcare systems and, ultimately, patient safety."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
